{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b517ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import torch; torch.set_default_dtype(torch.float64)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import mymodule as myModules\n",
    "import matplotlib.cm as cm\n",
    "from scipy import linalg\n",
    "from timeit import default_timer as timer\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer: general trainer that just computes a loss over a training set and\n",
    "        runs an evaluation on a validation test\n",
    "        \n",
    "    Initialization:\n",
    "        \n",
    "        model (Modules.model class): model to train\n",
    "        data (Utils.data class): needs to have a getSamples and an evaluate\n",
    "            method\n",
    "        nEpochs (int): number of epochs (passes over the dataset)\n",
    "        batchSize (int): size of each minibatch\n",
    "\n",
    "        Optional (keyword) arguments:\n",
    "            \n",
    "        validationInterval (int): interval of training (number of training\n",
    "            steps) without running a validation stage.\n",
    "\n",
    "        learningRateDecayRate (float): float that multiplies the latest learning\n",
    "            rate used.\n",
    "        learningRateDecayPeriod (int): how many training steps before \n",
    "            multiplying the learning rate decay rate by the actual learning\n",
    "            rate.\n",
    "        > Obs.: Both of these have to be defined for the learningRateDecay\n",
    "              scheduler to be activated.\n",
    "        logger (Visualizer): save tensorboard logs.\n",
    "        saveDir (string): path to the directory where to save relevant training\n",
    "            variables.\n",
    "        printInterval (int): how many training steps after which to print\n",
    "            partial results (0 means do not print)\n",
    "        graphNo (int): keep track of what graph realization this is\n",
    "        realitizationNo (int): keep track of what data realization this is\n",
    "        >> Alternatively, these last two keyword arguments can be used to keep\n",
    "            track of different trainings of the same model\n",
    "            \n",
    "    Training:\n",
    "        \n",
    "        .train(): trains the model and returns trainVars dict with the keys\n",
    "            'nEpochs': number of epochs (int)\n",
    "            'nBatches': number of batches (int)\n",
    "            'validationInterval': number of training steps in between \n",
    "                validation steps (int)\n",
    "            'batchSize': batch size of each training step (np.array)\n",
    "            'batchIndex': indices for the start sample and end sample of each\n",
    "                batch (np.array)\n",
    "            'lossTrain': loss function on the training samples for each training\n",
    "                step (np.array)\n",
    "            'evalTrain': evaluation function on the training samples for each\n",
    "                training step (np.array)\n",
    "            'lossValid': loss function on the validation samples for each\n",
    "                validation step (np.array)\n",
    "            'evalValid': evaluation function on the validation samples for each\n",
    "                validation step (np.array)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, data, nEpochs, batchSize, **kwargs):\n",
    "        \n",
    "        #\\\\\\ Store model\n",
    "        \n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        \n",
    "        ####################################\n",
    "        # ARGUMENTS (Store chosen options) #\n",
    "        ####################################\n",
    "        \n",
    "        # Training Options:\n",
    "        if 'doLogging' in kwargs.keys():\n",
    "            doLogging = kwargs['doLogging']\n",
    "        else:\n",
    "            doLogging = False\n",
    "\n",
    "        if 'doSaveVars' in kwargs.keys():\n",
    "            doSaveVars = kwargs['doSaveVars']\n",
    "        else:\n",
    "            doSaveVars = True\n",
    "\n",
    "        if 'printInterval' in kwargs.keys():\n",
    "            printInterval = kwargs['printInterval']\n",
    "            if printInterval > 0:\n",
    "                doPrint = True\n",
    "            else:\n",
    "                doPrint = False\n",
    "        else:\n",
    "            doPrint = True\n",
    "            printInterval = (data.nTrain//batchSize)//5\n",
    "\n",
    "        if 'learningRateDecayRate' in kwargs.keys() and \\\n",
    "            'learningRateDecayPeriod' in kwargs.keys():\n",
    "            doLearningRateDecay = True\n",
    "            learningRateDecayRate = kwargs['learningRateDecayRate']\n",
    "            learningRateDecayPeriod = kwargs['learningRateDecayPeriod']\n",
    "        else:\n",
    "            doLearningRateDecay = False\n",
    "\n",
    "        if 'validationInterval' in kwargs.keys():\n",
    "            validationInterval = kwargs['validationInterval']\n",
    "        else:\n",
    "            validationInterval = data.nTrain//batchSize\n",
    "\n",
    "        if 'earlyStoppingLag' in kwargs.keys():\n",
    "            doEarlyStopping = True\n",
    "            earlyStoppingLag = kwargs['earlyStoppingLag']\n",
    "        else:\n",
    "            doEarlyStopping = False\n",
    "            earlyStoppingLag = 0\n",
    "\n",
    "        if 'graphNo' in kwargs.keys():\n",
    "            graphNo = kwargs['graphNo']\n",
    "        else:\n",
    "            graphNo = -1\n",
    "\n",
    "        if 'realizationNo' in kwargs.keys():\n",
    "            if 'graphNo' in kwargs.keys():\n",
    "                realizationNo = kwargs['realizationNo']\n",
    "            else:\n",
    "                graphNo = kwargs['realizationNo']\n",
    "                realizationNo = -1\n",
    "        else:\n",
    "            realizationNo = -1\n",
    "\n",
    "        if doLogging:\n",
    "            from alegnn.utils.visualTools import Visualizer\n",
    "            logsTB = os.path.join(self.saveDir, self.name + '-logsTB')\n",
    "            logger = Visualizer(logsTB, name='visualResults')\n",
    "        else:\n",
    "            logger = None\n",
    "        \n",
    "        # No training case:\n",
    "        if nEpochs == 0:\n",
    "            doSaveVars = False\n",
    "            doLogging = False\n",
    "            # If there's no training happening, there's nothing to report about\n",
    "            # training losses and stuff.\n",
    "            \n",
    "        ###########################################\n",
    "        # DATA INPUT (pick up on data parameters) #\n",
    "        ###########################################\n",
    "\n",
    "        nTrain = data.nTrain # size of the training set\n",
    "\n",
    "        # Number of batches: If the desired number of batches does not split the\n",
    "        # dataset evenly, we reduce the size of the last batch (the number of\n",
    "        # samples in the last batch).\n",
    "        # The variable batchSize is a list of length nBatches (number of\n",
    "        # batches), where each element of the list is a number indicating the\n",
    "        # size of the corresponding batch.\n",
    "        if nTrain < batchSize:\n",
    "            nBatches = 1\n",
    "            batchSize = [nTrain]\n",
    "        elif nTrain % batchSize != 0:\n",
    "            nBatches = np.ceil(nTrain/batchSize).astype(np.int64)\n",
    "            batchSize = [batchSize] * nBatches\n",
    "            # If the sum of all batches so far is not the total number of\n",
    "            # graphs, start taking away samples from the last batch (remember\n",
    "            # that we used ceiling, so we are overshooting with the estimated\n",
    "            # number of batches)\n",
    "            while sum(batchSize) != nTrain:\n",
    "                batchSize[-1] -= 1\n",
    "        # If they fit evenly, then just do so.\n",
    "        else:\n",
    "            nBatches = np.int(nTrain/batchSize)\n",
    "            batchSize = [batchSize] * nBatches\n",
    "        # batchIndex is used to determine the first and last element of each\n",
    "        # batch.\n",
    "        # If batchSize is, for example [20,20,20] meaning that there are three\n",
    "        # batches of size 20 each, then cumsum will give [20,40,60] which\n",
    "        # determines the last index of each batch: up to 20, from 20 to 40, and\n",
    "        # from 40 to 60. We add the 0 at the beginning so that\n",
    "        # batchIndex[b]:batchIndex[b+1] gives the right samples for batch b.\n",
    "        batchIndex = np.cumsum(batchSize).tolist()\n",
    "        batchIndex = [0] + batchIndex\n",
    "        \n",
    "        ###################\n",
    "        # SAVE ATTRIBUTES #\n",
    "        ###################\n",
    "\n",
    "        self.trainingOptions = {}\n",
    "        self.trainingOptions['doLogging'] = doLogging\n",
    "        self.trainingOptions['logger'] = logger\n",
    "        self.trainingOptions['doSaveVars'] = doSaveVars\n",
    "        self.trainingOptions['doPrint'] = doPrint\n",
    "        self.trainingOptions['printInterval'] = printInterval\n",
    "        self.trainingOptions['doLearningRateDecay'] = doLearningRateDecay\n",
    "        if doLearningRateDecay:\n",
    "            self.trainingOptions['learningRateDecayRate'] = \\\n",
    "                                                         learningRateDecayRate\n",
    "            self.trainingOptions['learningRateDecayPeriod'] = \\\n",
    "                                                         learningRateDecayPeriod\n",
    "        self.trainingOptions['validationInterval'] = validationInterval\n",
    "        self.trainingOptions['doEarlyStopping'] = doEarlyStopping\n",
    "        self.trainingOptions['earlyStoppingLag'] = earlyStoppingLag\n",
    "        self.trainingOptions['batchIndex'] = batchIndex\n",
    "        self.trainingOptions['batchSize'] = batchSize\n",
    "        self.trainingOptions['nEpochs'] = nEpochs\n",
    "        self.trainingOptions['nBatches'] = nBatches\n",
    "        self.trainingOptions['graphNo'] = graphNo\n",
    "        self.trainingOptions['realizationNo'] = realizationNo\n",
    "        \n",
    "    def trainBatch(self, thisBatchIndices):\n",
    "        \n",
    "        # Get the samples\n",
    "        xTrain, yTrain = self.data.getSamples('train', thisBatchIndices)\n",
    "        xTrain = xTrain.to(self.model.device)\n",
    "        yTrain = yTrain.to(self.model.device)\n",
    "\n",
    "        # Start measuring time\n",
    "        startTime = datetime.datetime.now()\n",
    "\n",
    "        # Reset gradients\n",
    "        self.model.archit.zero_grad()\n",
    "\n",
    "        # Obtain the output of the GNN\n",
    "        yHatTrain = self.model.archit(xTrain)\n",
    "\n",
    "        # Compute loss\n",
    "        lossValueTrain = self.model.loss(yHatTrain, yTrain)\n",
    "\n",
    "        # Compute gradients\n",
    "        lossValueTrain.backward()\n",
    "\n",
    "        # Optimize\n",
    "        self.model.optim.step()\n",
    "\n",
    "        # Finish measuring time\n",
    "        endTime = datetime.datetime.now()\n",
    "\n",
    "        timeElapsed = abs(endTime - startTime).total_seconds()\n",
    "\n",
    "        # Compute the accuracy\n",
    "        #   Note: Using yHatTrain.data creates a new tensor with the\n",
    "        #   same value, but detaches it from the gradient, so that no\n",
    "        #   gradient operation is taken into account here.\n",
    "        #   (Alternatively, we could use a with torch.no_grad():)\n",
    "        costTrain = self.data.evaluate(yHatTrain.data, yTrain)\n",
    "        \n",
    "        return lossValueTrain.item(), costTrain.item(), timeElapsed\n",
    "    \n",
    "    def validationStep(self):\n",
    "        \n",
    "        # Validation:\n",
    "        xValid, yValid = self.data.getSamples('valid')\n",
    "        xValid = xValid.to(self.model.device)\n",
    "        yValid = yValid.to(self.model.device)\n",
    "\n",
    "        # Start measuring time\n",
    "        startTime = datetime.datetime.now()\n",
    "\n",
    "        # Under torch.no_grad() so that the computations carried out\n",
    "        # to obtain the validation accuracy are not taken into\n",
    "        # account to update the learnable parameters.\n",
    "        with torch.no_grad():\n",
    "            # Obtain the output of the GNN\n",
    "            yHatValid = self.model.archit(xValid)\n",
    "\n",
    "            # Compute loss\n",
    "            lossValueValid = self.model.loss(yHatValid, yValid)\n",
    "\n",
    "            # Finish measuring time\n",
    "            endTime = datetime.datetime.now()\n",
    "\n",
    "            timeElapsed = abs(endTime - startTime).total_seconds()\n",
    "\n",
    "            # Compute accuracy:\n",
    "            costValid = self.data.evaluate(yHatValid, yValid)\n",
    "        \n",
    "        return lossValueValid.item(), costValid.item(), timeElapsed\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # Get back the training options\n",
    "        assert 'trainingOptions' in dir(self)\n",
    "        assert 'doLogging' in self.trainingOptions.keys()\n",
    "        doLogging = self.trainingOptions['doLogging']\n",
    "        assert 'logger' in self.trainingOptions.keys()\n",
    "        logger = self.trainingOptions['logger']\n",
    "        assert 'doSaveVars' in self.trainingOptions.keys()\n",
    "        doSaveVars = self.trainingOptions['doSaveVars']\n",
    "        assert 'doPrint' in self.trainingOptions.keys()\n",
    "        doPrint = self.trainingOptions['doPrint']\n",
    "        assert 'printInterval' in self.trainingOptions.keys()\n",
    "        printInterval = self.trainingOptions['printInterval']\n",
    "        assert 'doLearningRateDecay' in self.trainingOptions.keys()\n",
    "        doLearningRateDecay = self.trainingOptions['doLearningRateDecay']\n",
    "        if doLearningRateDecay:\n",
    "            assert 'learningRateDecayRate' in self.trainingOptions.keys()\n",
    "            learningRateDecayRate=self.trainingOptions['learningRateDecayRate']\n",
    "            assert 'learningRateDecayPeriod' in self.trainingOptions.keys()\n",
    "            learningRateDecayPeriod=self.trainingOptions['learningRateDecayPeriod']\n",
    "        assert 'validationInterval' in self.trainingOptions.keys()\n",
    "        validationInterval = self.trainingOptions['validationInterval']\n",
    "        assert 'doEarlyStopping' in self.trainingOptions.keys()\n",
    "        doEarlyStopping = self.trainingOptions['doEarlyStopping']\n",
    "        assert 'earlyStoppingLag' in self.trainingOptions.keys()\n",
    "        earlyStoppingLag = self.trainingOptions['earlyStoppingLag']\n",
    "        assert 'batchIndex' in self.trainingOptions.keys()\n",
    "        batchIndex = self.trainingOptions['batchIndex']\n",
    "        assert 'batchSize' in self.trainingOptions.keys()\n",
    "        batchSize = self.trainingOptions['batchSize']\n",
    "        assert 'nEpochs' in self.trainingOptions.keys()\n",
    "        nEpochs = self.trainingOptions['nEpochs']\n",
    "        assert 'nBatches' in self.trainingOptions.keys()\n",
    "        nBatches = self.trainingOptions['nBatches']\n",
    "        assert 'graphNo' in self.trainingOptions.keys()\n",
    "        graphNo = self.trainingOptions['graphNo']\n",
    "        assert 'realizationNo' in self.trainingOptions.keys()\n",
    "        realizationNo = self.trainingOptions['realizationNo']\n",
    "        \n",
    "        # Learning rate scheduler:\n",
    "        if doLearningRateDecay:\n",
    "            learningRateScheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                 self.model.optim,learningRateDecayPeriod,learningRateDecayRate)\n",
    "\n",
    "        # Initialize counters (since we give the possibility of early stopping,\n",
    "        # we had to drop the 'for' and use a 'while' instead):\n",
    "        epoch = 0 # epoch counter\n",
    "        lagCount = 0 # lag counter for early stopping\n",
    "        \n",
    "        # Store the training variables\n",
    "        lossTrain = []\n",
    "        costTrain = []\n",
    "        lossValid = []\n",
    "        costValid = []\n",
    "        timeTrain = []\n",
    "        timeValid = []\n",
    "\n",
    "        while epoch < nEpochs \\\n",
    "                    and (lagCount < earlyStoppingLag or (not doEarlyStopping)):\n",
    "            # The condition will be zero (stop), whenever one of the items of\n",
    "            # the 'and' is zero. Therefore, we want this to stop only for epoch\n",
    "            # counting when we are NOT doing early stopping. This can be\n",
    "            # achieved if the second element of the 'and' is always 1 (so that\n",
    "            # the first element, the epoch counting, decides). In order to\n",
    "            # force the second element to be one whenever there is not early\n",
    "            # stopping, we have an or, and force it to one. So, when we are not\n",
    "            # doing early stopping, the variable 'not doEarlyStopping' is 1,\n",
    "            # and the result of the 'or' is 1 regardless of the lagCount. When\n",
    "            # we do early stopping, then the variable 'not doEarlyStopping' is\n",
    "            # 0, and the value 1 for the 'or' gate is determined by the lag\n",
    "            # count.\n",
    "            # ALTERNATIVELY, we could just keep 'and lagCount<earlyStoppingLag'\n",
    "            # and be sure that lagCount can only be increased whenever\n",
    "            # doEarlyStopping is True. But I somehow figured out that would be\n",
    "            # harder to maintain (more parts of the code to check if we are\n",
    "            # accidentally increasing lagCount).\n",
    "\n",
    "            # Randomize dataset for each epoch\n",
    "            randomPermutation = np.random.permutation(self.data.nTrain)\n",
    "            # Convert a numpy.array of numpy.int into a list of actual int.\n",
    "            idxEpoch = [int(i) for i in randomPermutation]\n",
    "\n",
    "            # Learning decay\n",
    "            if doLearningRateDecay:\n",
    "                learningRateScheduler.step()\n",
    "\n",
    "                if doPrint:\n",
    "                    # All the optimization have the same learning rate, so just\n",
    "                    # print one of them\n",
    "                    # TODO: Actually, they might be different, so I will need to\n",
    "                    # print all of them.\n",
    "                    print(\"Epoch %d, learning rate = %.8f\" % (epoch+1,\n",
    "                          learningRateScheduler.optim.param_groups[0]['lr']))\n",
    "\n",
    "            # Initialize counter\n",
    "            batch = 0 # batch counter\n",
    "            while batch < nBatches \\\n",
    "                        and (lagCount<earlyStoppingLag or (not doEarlyStopping)):\n",
    "\n",
    "                # Extract the adequate batch\n",
    "                thisBatchIndices = idxEpoch[batchIndex[batch]\n",
    "                                            : batchIndex[batch+1]]\n",
    "                \n",
    "                lossValueTrain, costValueTrain, timeElapsed = \\\n",
    "                                               self.trainBatch(thisBatchIndices)\n",
    "                \n",
    "\n",
    "                # Logging values\n",
    "                if doLogging:\n",
    "                    lossTrainTB = lossValueTrain\n",
    "                    costTrainTB = costValueTrain\n",
    "                # Save values\n",
    "                lossTrain += [lossValueTrain]\n",
    "                costTrain += [costValueTrain]\n",
    "                timeTrain += [timeElapsed]\n",
    "\n",
    "                # Print:\n",
    "                if doPrint:\n",
    "                    if (epoch * nBatches + batch) % printInterval == 0:\n",
    "                        print(\"\\t(E: %2d, B: %3d) %6.4f / %7.4f - %6.4fs\" % (\n",
    "                                epoch+1, batch+1, costValueTrain,\n",
    "                                lossValueTrain, timeElapsed),\n",
    "                            end = ' ')\n",
    "                        if graphNo > -1:\n",
    "                            print(\"[%d\" % graphNo, end = '')\n",
    "                            if realizationNo > -1:\n",
    "                                print(\"/%d\" % realizationNo,\n",
    "                                      end = '')\n",
    "                            print(\"]\", end = '')\n",
    "                        print(\"\")\n",
    "\n",
    "                #\\\\\\\\\\\\\\\n",
    "                #\\\\\\ TB LOGGING (for each batch)\n",
    "                #\\\\\\\\\\\\\\\n",
    "\n",
    "                if doLogging:\n",
    "                    logger.scalar_summary(mode = 'Training',\n",
    "                                          epoch = epoch * nBatches + batch,\n",
    "                                          **{'lossTrain': lossTrainTB,\n",
    "                                           'costTrain': costTrainTB})\n",
    "\n",
    "                #\\\\\\\\\\\\\\\n",
    "                #\\\\\\ VALIDATION\n",
    "                #\\\\\\\\\\\\\\\n",
    "\n",
    "                if (epoch * nBatches + batch) % validationInterval == 0:\n",
    "\n",
    "                    lossValueValid, costValueValid, timeElapsed = \\\n",
    "                                                           self.validationStep()\n",
    "\n",
    "                    # Logging values\n",
    "                    if doLogging:\n",
    "                        lossValidTB = lossValueValid\n",
    "                        costValidTB = costValueValid\n",
    "                    # Save values\n",
    "                    lossValid += [lossValueValid]\n",
    "                    costValid += [costValueValid]\n",
    "                    timeValid += [timeElapsed]\n",
    "\n",
    "                    # Print:\n",
    "                    if doPrint:\n",
    "                        print(\"\\t(E: %2d, B: %3d) %6.4f / %7.4f - %6.4fs\" % (\n",
    "                                epoch+1, batch+1,\n",
    "                                costValueValid, \n",
    "                                lossValueValid,\n",
    "                                timeElapsed), end = ' ')\n",
    "                        print(\"[VALIDATION\", end = '')\n",
    "                        if graphNo > -1:\n",
    "                            print(\".%d\" % graphNo, end = '')\n",
    "                            if realizationNo > -1:\n",
    "                                print(\"/%d\" % realizationNo, end = '')\n",
    "                        print(\" (%s)]\" % self.model.name)\n",
    "\n",
    "\n",
    "                    if doLogging:\n",
    "                        logger.scalar_summary(mode = 'Validation',\n",
    "                                          epoch = epoch * nBatches + batch,\n",
    "                                          **{'lossValid': lossValidTB,\n",
    "                                           'costValid': costValidTB})\n",
    "\n",
    "                    # No previous best option, so let's record the first trial\n",
    "                    # as the best option\n",
    "                    if epoch == 0 and batch == 0:\n",
    "                        bestScore = costValueValid\n",
    "                        bestEpoch, bestBatch = epoch, batch\n",
    "                        # Save this model as the best (so far)\n",
    "                        self.model.save(label = 'Best')\n",
    "                        # Start the counter\n",
    "                        if doEarlyStopping:\n",
    "                            initialBest = True\n",
    "                    else:\n",
    "                        thisValidScore = costValueValid\n",
    "                        if thisValidScore < bestScore:\n",
    "                            bestScore = thisValidScore\n",
    "                            bestEpoch, bestBatch = epoch, batch\n",
    "                            if doPrint:\n",
    "                                print(\"\\t=> New best achieved: %.4f\" % \\\n",
    "                                          (bestScore))\n",
    "                            self.model.save(label = 'Best')\n",
    "                            # Now that we have found a best that is not the\n",
    "                            # initial one, we can start counting the lag (if\n",
    "                            # needed)\n",
    "                            initialBest = False\n",
    "                            # If we achieved a new best, then we need to reset\n",
    "                            # the lag count.\n",
    "                            if doEarlyStopping:\n",
    "                                lagCount = 0\n",
    "                        # If we didn't achieve a new best, increase the lag\n",
    "                        # count.\n",
    "                        # Unless it was the initial best, in which case we\n",
    "                        # haven't found any best yet, so we shouldn't be doing\n",
    "                        # the early stopping count.\n",
    "                        elif doEarlyStopping and not initialBest:\n",
    "                            lagCount += 1\n",
    "\n",
    "                #\\\\\\\\\\\\\\\n",
    "                #\\\\\\ END OF BATCH:\n",
    "                #\\\\\\\\\\\\\\\n",
    "\n",
    "                #\\\\\\ Increase batch count:\n",
    "                batch += 1\n",
    "\n",
    "            #\\\\\\\\\\\\\\\n",
    "            #\\\\\\ END OF EPOCH:\n",
    "            #\\\\\\\\\\\\\\\n",
    "\n",
    "            #\\\\\\ Increase epoch count:\n",
    "            epoch += 1\n",
    "\n",
    "        #\\\\\\ Save models:\n",
    "        self.model.save(label = 'Last')\n",
    "\n",
    "        #################\n",
    "        # TRAINING OVER #\n",
    "        #################\n",
    "\n",
    "        # We convert the lists into np.arrays\n",
    "        lossTrain = np.array(lossTrain)\n",
    "        costTrain = np.array(costTrain)\n",
    "        lossValid = np.array(lossValid)\n",
    "        costValid = np.array(costValid)\n",
    "        # And we would like to save all the relevant information from\n",
    "        # training\n",
    "        trainVars = {'nEpochs': nEpochs,\n",
    "                     'nBatches': nBatches,\n",
    "                     'validationInterval': validationInterval,\n",
    "                     'batchSize': np.array(batchSize),\n",
    "                     'batchIndex': np.array(batchIndex),\n",
    "                     'lossTrain': lossTrain,\n",
    "                     'costTrain': costTrain,\n",
    "                     'lossValid': lossValid,\n",
    "                     'costValid': costValid\n",
    "                     }\n",
    "        \n",
    "        if doSaveVars:\n",
    "            saveDirVars = os.path.join(self.model.saveDir, 'trainVars')\n",
    "            if not os.path.exists(saveDirVars):\n",
    "                os.makedirs(saveDirVars)\n",
    "            pathToFile = os.path.join(saveDirVars,\n",
    "                                      self.model.name + 'trainVars.pkl')\n",
    "            with open(pathToFile, 'wb') as trainVarsFile:\n",
    "                pickle.dump(trainVars, trainVarsFile)\n",
    "\n",
    "        # Now, if we didn't do any training (i.e. nEpochs = 0), then the last is\n",
    "        # also the best.\n",
    "        if nEpochs == 0:\n",
    "            self.model.save(label = 'Best')\n",
    "            self.model.save(label = 'Last')\n",
    "            if doPrint:\n",
    "                print(\"WARNING: No training. Best and Last models are the same.\")\n",
    "\n",
    "        # After training is done, reload best model before proceeding to\n",
    "        # evaluation:\n",
    "        self.model.load(label = 'Best')\n",
    "\n",
    "        #\\\\\\ Print out best:\n",
    "        if doPrint and nEpochs > 0:\n",
    "            print(\"=> Best validation achieved (E: %d, B: %d): %.4f\" % (\n",
    "                    bestEpoch + 1, bestBatch + 1, bestScore))\n",
    "            \n",
    "        return trainVars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
